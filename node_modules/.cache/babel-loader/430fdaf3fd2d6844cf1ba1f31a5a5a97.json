{"ast":null,"code":"'use strict';\n\nconst {\n  DAGLink,\n  DAGNode\n} = require('ipld-dag-pb');\n\nconst CID = require('cids');\n\nconst log = require('debug')('ipfs:mfs:core:utils:add-link');\n\nconst UnixFS = require('ipfs-unixfs');\n\nconst DirSharded = require('ipfs-unixfs-importer/src/dir-sharded');\n\nconst {\n  updateHamtDirectory,\n  recreateHamtLevel,\n  createShard,\n  toPrefix,\n  addLinksToHamtBucket\n} = require('./hamt-utils');\n\nconst errCode = require('err-code');\n\nconst mc = require('multicodec');\n\nconst mh = require('multihashes');\n\nconst last = require('it-last');\n\nconst addLink = async (context, options) => {\n  if (!options.parentCid && !options.parent) {\n    throw errCode(new Error('No parent node or CID passed to addLink'), 'EINVALIDPARENT');\n  }\n\n  if (options.parentCid && !CID.isCID(options.parentCid)) {\n    throw errCode(new Error('Invalid CID passed to addLink'), 'EINVALIDPARENTCID');\n  }\n\n  if (!options.parent) {\n    log(`Loading parent node ${options.parentCid}`);\n    options.parent = await context.ipld.get(options.parentCid);\n  }\n\n  if (!options.cid) {\n    throw errCode(new Error('No child cid passed to addLink'), 'EINVALIDCHILDCID');\n  }\n\n  if (!options.name) {\n    throw errCode(new Error('No child name passed to addLink'), 'EINVALIDCHILDNAME');\n  }\n\n  if (!CID.isCID(options.cid)) {\n    options.cid = new CID(options.cid);\n  }\n\n  if (!options.size && options.size !== 0) {\n    throw errCode(new Error('No child size passed to addLink'), 'EINVALIDCHILDSIZE');\n  }\n\n  const meta = UnixFS.unmarshal(options.parent.Data);\n\n  if (meta.type === 'hamt-sharded-directory') {\n    log('Adding link to sharded directory');\n    return addToShardedDirectory(context, options);\n  }\n\n  if (options.parent.Links.length >= options.shardSplitThreshold) {\n    log('Converting directory to sharded directory');\n    return convertToShardedDirectory(context, { ...options,\n      mtime: meta.mtime,\n      mode: meta.mode\n    });\n  }\n\n  log(`Adding ${options.name} (${options.cid}) to regular directory`);\n  return addToDirectory(context, options);\n};\n\nconst convertToShardedDirectory = async (context, options) => {\n  const result = await createShard(context, options.parent.Links.map(link => ({\n    name: link.Name,\n    size: link.Tsize,\n    cid: link.Hash\n  })).concat({\n    name: options.name,\n    size: options.size,\n    cid: options.cid\n  }), options);\n  log(`Converted directory to sharded directory ${result.cid}`);\n  return result;\n};\n\nconst addToDirectory = async (context, options) => {\n  options.parent.rmLink(options.name);\n  options.parent.addLink(new DAGLink(options.name, options.size, options.cid));\n  const node = UnixFS.unmarshal(options.parent.Data);\n\n  if (node.mtime) {\n    // Update mtime if previously set\n    node.mtime = new Date();\n    options.parent = new DAGNode(node.marshal(), options.parent.Links);\n  }\n\n  const hashAlg = mh.names[options.hashAlg]; // Persist the new parent DAGNode\n\n  const cid = await context.ipld.put(options.parent, mc.DAG_PB, {\n    cidVersion: options.cidVersion,\n    hashAlg,\n    onlyHash: !options.flush\n  });\n  return {\n    node: options.parent,\n    cid,\n    size: options.parent.size\n  };\n};\n\nconst addToShardedDirectory = async (context, options) => {\n  const {\n    shard,\n    path\n  } = await addFileToShardedDirectory(context, options);\n  const result = await last(shard.flush('', context.block));\n  const node = await context.ipld.get(result.cid); // we have written out the shard, but only one sub-shard will have been written so replace it in the original shard\n\n  const oldLink = options.parent.Links.find(link => link.Name.substring(0, 2) === path[0].prefix);\n  const newLink = node.Links.find(link => link.Name.substring(0, 2) === path[0].prefix);\n\n  if (oldLink) {\n    options.parent.rmLink(oldLink.Name);\n  }\n\n  options.parent.addLink(newLink);\n  return updateHamtDirectory(context, options.parent.Links, path[0].bucket, options);\n};\n\nconst addFileToShardedDirectory = async (context, options) => {\n  const file = {\n    name: options.name,\n    cid: options.cid,\n    size: options.size\n  }; // start at the root bucket and descend, loading nodes as we go\n\n  const rootBucket = await recreateHamtLevel(options.parent.Links);\n  const node = UnixFS.unmarshal(options.parent.Data);\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: null,\n    parentKey: null,\n    path: '',\n    dirty: true,\n    flat: false,\n    mode: node.mode\n  }, options);\n  shard._bucket = rootBucket;\n\n  if (node.mtime) {\n    // update mtime if previously set\n    shard.mtime = new Date();\n  } // load subshards until the bucket & position no longer changes\n\n\n  const position = await rootBucket._findNewBucketAndPos(file.name);\n  const path = toBucketPath(position);\n  path[0].node = options.parent;\n  let index = 0;\n\n  while (index < path.length) {\n    const segment = path[index];\n    index++;\n    const node = segment.node;\n    const link = node.Links.find(link => link.Name.substring(0, 2) === segment.prefix);\n\n    if (!link) {\n      // prefix is new, file will be added to the current bucket\n      log(`Link ${segment.prefix}${file.name} will be added`);\n      index = path.length;\n      break;\n    }\n\n    if (link.Name === `${segment.prefix}${file.name}`) {\n      // file already existed, file will be added to the current bucket\n      log(`Link ${segment.prefix}${file.name} will be replaced`);\n      index = path.length;\n      break;\n    }\n\n    if (link.Name.length > 2) {\n      // another file had the same prefix, will be replaced with a subshard\n      log(`Link ${link.Name} ${link.Hash} will be replaced with a subshard`);\n      index = path.length;\n      break;\n    } // load sub-shard\n\n\n    log(`Found subshard ${segment.prefix}`);\n    const subShard = await context.ipld.get(link.Hash); // subshard hasn't been loaded, descend to the next level of the HAMT\n\n    if (!path[index]) {\n      log(`Loaded new subshard ${segment.prefix}`);\n      await recreateHamtLevel(subShard.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n      const position = await rootBucket._findNewBucketAndPos(file.name);\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: subShard\n      });\n      break;\n    }\n\n    const nextSegment = path[index]; // add next levels worth of links to bucket\n\n    await addLinksToHamtBucket(subShard.Links, nextSegment.bucket, rootBucket);\n    nextSegment.node = subShard;\n  } // finally add the new file into the shard\n\n\n  await shard._bucket.put(file.name, {\n    size: file.size,\n    cid: file.cid\n  });\n  return {\n    shard,\n    path\n  };\n};\n\nconst toBucketPath = position => {\n  let bucket = position.bucket;\n  let positionInBucket = position.pos;\n  const path = [{\n    bucket,\n    prefix: toPrefix(positionInBucket)\n  }];\n  bucket = position.bucket._parent;\n  positionInBucket = position.bucket._posAtParent;\n\n  while (bucket) {\n    path.push({\n      bucket,\n      prefix: toPrefix(positionInBucket)\n    });\n    positionInBucket = bucket._posAtParent;\n    bucket = bucket._parent;\n  }\n\n  path.reverse();\n  return path;\n};\n\nmodule.exports = addLink;","map":null,"metadata":{},"sourceType":"script"}