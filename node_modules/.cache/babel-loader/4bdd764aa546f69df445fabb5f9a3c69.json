{"ast":null,"code":"'use strict';\n\nconst pMap = require('p-map');\n\nconst pDoWhilst = require('p-do-whilst');\n\nconst Entry = require('./entry');\n\nconst hasItems = arr => arr && arr.length > 0;\n\nclass EntryIO {\n  // Fetch log graphs in parallel\n  static async fetchParallel(ipfs, hashes, {\n    length,\n    exclude = [],\n    timeout,\n    concurrency,\n    onProgressCallback\n  }) {\n    const fetchOne = async hash => EntryIO.fetchAll(ipfs, hash, {\n      length,\n      exclude,\n      timeout,\n      onProgressCallback,\n      concurrency\n    });\n\n    const concatArrays = (arr1, arr2) => arr1.concat(arr2);\n\n    const flatten = arr => arr.reduce(concatArrays, []);\n\n    const res = await pMap(hashes, fetchOne, {\n      concurrency: Math.max(concurrency || hashes.length, 1)\n    });\n    return flatten(res);\n  }\n  /**\n   * Fetch log entries\n   *\n   * @param {IPFS} [ipfs] An IPFS instance\n   * @param {string} [hash] Multihash of the entry to fetch\n   * @param {string} [parent] Parent of the node to be fetched\n   * @param {Object} [all] Entries to skip\n   * @param {Number} [amount=-1] How many entries to fetch\n   * @param {Number} [depth=0] Current depth of the recursion\n   * @param {function(hash, entry, parent, depth)} onProgressCallback\n   * @returns {Promise<Array<Entry>>}\n   */\n\n\n  static async fetchAll(ipfs, hashes, {\n    length = -1,\n    exclude = [],\n    timeout,\n    onProgressCallback,\n    onStartProgressCallback,\n    concurrency = 32,\n    delay = 0\n  } = {}) {\n    const result = [];\n    const cache = {};\n    const loadingCache = {};\n    const loadingQueue = Array.isArray(hashes) ? {\n      0: hashes.slice()\n    } : {\n      0: [hashes]\n    };\n    let running = 0; // keep track of how many entries are being fetched at any time\n\n    let maxClock = 0; // keep track of the latest clock time during load\n\n    let minClock = 0; // keep track of the minimum clock time during load\n    // Does the loading queue have more to process?\n\n    const loadingQueueHasMore = () => Object.values(loadingQueue).find(hasItems) !== undefined; // Add a multihash to the loading queue\n\n\n    const addToLoadingQueue = (e, idx) => {\n      if (!loadingCache[e]) {\n        if (!loadingQueue[idx]) loadingQueue[idx] = [];\n\n        if (!loadingQueue[idx].includes(e)) {\n          loadingQueue[idx].push(e);\n        }\n\n        loadingCache[e] = true;\n      }\n    }; // Get the next items to process from the loading queue\n\n\n    const getNextFromQueue = (length = 1) => {\n      const getNext = (res, key, idx) => {\n        const nextItems = loadingQueue[key];\n\n        while (nextItems.length > 0 && res.length < length) {\n          const hash = nextItems.shift();\n          res.push(hash);\n        }\n\n        if (nextItems.length === 0) {\n          delete loadingQueue[key];\n        }\n\n        return res;\n      };\n\n      return Object.keys(loadingQueue).reduce(getNext, []);\n    }; // Add entries that we don't need to fetch to the \"cache\"\n\n\n    const addToExcludeCache = e => {\n      cache[e.hash] = true;\n    }; // Fetch one entry and add it to the results\n\n\n    const fetchEntry = async hash => {\n      if (!hash || cache[hash]) {\n        return;\n      }\n\n      return new Promise((resolve, reject) => {\n        // Resolve the promise after a timeout (if given) in order to\n        // not get stuck loading a block that is unreachable\n        const timer = timeout && timeout > 0 ? setTimeout(() => {\n          console.warn(`Warning: Couldn't fetch entry '${hash}', request timed out (${timeout}ms)`);\n          resolve();\n        }, timeout) : null;\n\n        const addToResults = entry => {\n          if (Entry.isEntry(entry)) {\n            const ts = entry.clock.time; // Update min/max clocks\n\n            maxClock = Math.max(maxClock, ts);\n            minClock = result.length > 0 ? Math.min(result[result.length - 1].clock.time, minClock) : maxClock;\n            const isLater = result.length >= length && ts >= minClock;\n\n            const calculateIndex = idx => maxClock - ts + (idx + 1) * idx; // Add the entry to the results if\n            // 1) we're fetching all entries\n            // 2) results is not filled yet\n            // the clock of the entry is later than current known minimum clock time\n\n\n            if (length < 0 || result.length < length || isLater) {\n              result.push(entry);\n              cache[hash] = true;\n\n              if (onProgressCallback) {\n                onProgressCallback(hash, entry, result.length, result.length);\n              }\n            }\n\n            if (length < 0) {\n              // If we're fetching all entries (length === -1), adds nexts and refs to the queue\n              entry.next.forEach(addToLoadingQueue);\n              if (entry.refs) entry.refs.forEach(addToLoadingQueue);\n            } else {\n              // If we're fetching entries up to certain length,\n              // fetch the next if result is filled up, to make sure we \"check\"\n              // the next entry if its clock is later than what we have in the result\n              if (result.length < length || ts > minClock || ts === minClock && !cache[entry.hash]) {\n                entry.next.forEach(e => addToLoadingQueue(e, calculateIndex(0)));\n              }\n\n              if (entry.refs && result.length + entry.refs.length <= length) {\n                entry.refs.forEach((e, i) => addToLoadingQueue(e, calculateIndex(i)));\n              }\n            }\n          }\n        };\n\n        if (onStartProgressCallback) {\n          onStartProgressCallback(hash, null, 0, result.length);\n        } // Load the entry\n\n\n        Entry.fromMultihash(ipfs, hash).then(async entry => {\n          try {\n            // Add it to the results\n            addToResults(entry); // Simulate network latency (for debugging purposes)\n\n            if (delay > 0) {\n              const sleep = (ms = 0) => new Promise(resolve => setTimeout(resolve, ms));\n\n              await sleep(delay);\n            }\n\n            resolve();\n          } catch (e) {\n            reject(e);\n          } finally {\n            clearTimeout(timer);\n          }\n        }).catch(reject);\n      });\n    }; // One loop of processing the loading queue\n\n\n    const _processQueue = async () => {\n      if (running < concurrency) {\n        const nexts = getNextFromQueue(concurrency);\n        running += nexts.length;\n        await pMap(nexts, fetchEntry);\n        running -= nexts.length;\n      }\n    }; // Add entries to exclude from processing to the cache before we start\n\n\n    exclude.forEach(addToExcludeCache); // Fetch entries\n\n    await pDoWhilst(_processQueue, loadingQueueHasMore);\n    return result;\n  }\n\n}\n\nmodule.exports = EntryIO;","map":null,"metadata":{},"sourceType":"script"}