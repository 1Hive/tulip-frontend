{"ast":null,"code":"'use strict';\n\nconst Entry = require('./entry');\n\nconst EntryIO = require('./entry-io');\n\nconst Sorting = require('./log-sorting');\n\nconst {\n  LastWriteWins,\n  NoZeroes\n} = Sorting;\n\nconst LogError = require('./log-errors');\n\nconst {\n  isDefined,\n  findUniques,\n  difference,\n  io\n} = require('./utils');\n\nconst IPLD_LINKS = ['heads'];\n\nconst last = (arr, n) => arr.slice(arr.length - Math.min(arr.length, n), arr.length);\n\nclass LogIO {\n  //\n\n  /**\n   * Get the multihash of a Log.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Log} log Log to get a multihash for\n   * @returns {Promise<string>}\n   * @deprecated\n   */\n  static async toMultihash(ipfs, log, {\n    format\n  } = {}) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError();\n    if (!isDefined(log)) throw LogError.LogNotDefinedError();\n    if (!isDefined(format)) format = 'dag-cbor';\n    if (log.values.length < 1) throw new Error('Can\\'t serialize an empty log');\n    return io.write(ipfs, format, log.toJSON(), {\n      links: IPLD_LINKS\n    });\n  }\n  /**\n   * Create a log from a hashes.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {string} hash The hash of the log\n   * @param {Object} options\n   * @param {number} options.length How many items to include in the log\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   */\n\n\n  static async fromMultihash(ipfs, hash, {\n    length = -1,\n    exclude = [],\n    timeout,\n    concurrency,\n    sortFn,\n    onProgressCallback\n  }) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError();\n    if (!isDefined(hash)) throw new Error(`Invalid hash: ${hash}`);\n    const logData = await io.read(ipfs, hash, {\n      links: IPLD_LINKS\n    });\n    if (!logData.heads || !logData.id) throw LogError.NotALogError(); // Use user provided sorting function or the default one\n\n    sortFn = sortFn || NoZeroes(LastWriteWins);\n\n    const isHead = e => logData.heads.includes(e.hash);\n\n    const all = await EntryIO.fetchAll(ipfs, logData.heads, {\n      length,\n      exclude,\n      timeout,\n      concurrency,\n      onProgressCallback\n    });\n    const logId = logData.id;\n    const entries = length > -1 ? last(all.sort(sortFn), length) : all;\n    const heads = entries.filter(isHead);\n    return {\n      logId,\n      entries,\n      heads\n    };\n  }\n  /**\n   * Create a log from an entry hash.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {string} hash The hash of the entry\n   * @param {Object} options\n   * @param {number} options.length How many items to include in the log\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   */\n\n\n  static async fromEntryHash(ipfs, hash, {\n    length = -1,\n    exclude = [],\n    timeout,\n    concurrency,\n    sortFn,\n    onProgressCallback\n  }) {\n    if (!isDefined(ipfs)) throw LogError.IpfsNotDefinedError();\n    if (!isDefined(hash)) throw new Error(\"'hash' must be defined\"); // Convert input hash(s) to an array\n\n    const hashes = Array.isArray(hash) ? hash : [hash]; // Fetch given length, return size at least the given input entries\n\n    length = length > -1 ? Math.max(length, 1) : length;\n    const all = await EntryIO.fetchParallel(ipfs, hashes, {\n      length,\n      exclude,\n      timeout,\n      concurrency,\n      onProgressCallback\n    }); // Cap the result at the right size by taking the last n entries,\n    // or if given length is -1, then take all\n\n    sortFn = sortFn || NoZeroes(LastWriteWins);\n    const entries = length > -1 ? last(all.sort(sortFn), length) : all;\n    return {\n      entries\n    };\n  }\n  /**\n   * Creates a log data from a JSON object, to be passed to a Log constructor\n   *\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {json} json A json object containing valid log data\n   * @param {Object} options\n   * @param {number} options.length How many entries to include\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   **/\n\n\n  static async fromJSON(ipfs, json, {\n    length = -1,\n    timeout,\n    concurrency,\n    onProgressCallback\n  }) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError();\n    const {\n      id,\n      heads\n    } = json;\n    const headHashes = heads.map(e => e.hash);\n    const all = await EntryIO.fetchParallel(ipfs, headHashes, {\n      length,\n      timeout,\n      concurrency,\n      onProgressCallback\n    });\n    const entries = all.sort(Entry.compare);\n    return {\n      logId: id,\n      entries,\n      heads\n    };\n  }\n  /**\n   * Create a new log starting from an entry.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Entry|Array<Entry>} sourceEntries An entry or an array of entries to fetch a log from\n   * @param {Object} options\n   * @param {number} options.length How many entries to include\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   */\n\n\n  static async fromEntry(ipfs, sourceEntries, {\n    length = -1,\n    exclude = [],\n    timeout,\n    concurrency,\n    onProgressCallback\n  }) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError();\n    if (!isDefined(sourceEntries)) throw new Error(\"'sourceEntries' must be defined\"); // Make sure we only have Entry objects as input\n\n    if (!Array.isArray(sourceEntries) && !Entry.isEntry(sourceEntries)) {\n      throw new Error('\\'sourceEntries\\' argument must be an array of Entry instances or a single Entry');\n    }\n\n    if (!Array.isArray(sourceEntries)) {\n      sourceEntries = [sourceEntries];\n    } // Fetch given length, return size at least the given input entries\n\n\n    length = length > -1 ? Math.max(length, sourceEntries.length) : length; // Make sure we pass hashes instead of objects to the fetcher function\n\n    const hashes = sourceEntries.map(e => e.hash); // Fetch the entries\n\n    const all = await EntryIO.fetchParallel(ipfs, hashes, {\n      length,\n      exclude,\n      timeout,\n      concurrency,\n      onProgressCallback\n    }); // Combine the fetches with the source entries and take only uniques\n\n    const combined = sourceEntries.concat(all).concat(exclude);\n    const uniques = findUniques(combined, 'hash').sort(Entry.compare); // Cap the result at the right size by taking the last n entries\n\n    const sliced = uniques.slice(length > -1 ? -length : -uniques.length); // Make sure that the given input entries are present in the result\n    // in order to not lose references\n\n    const missingSourceEntries = difference(sliced, sourceEntries, 'hash');\n\n    const replaceInFront = (a, withEntries) => {\n      var sliced = a.slice(withEntries.length, a.length);\n      return withEntries.concat(sliced);\n    }; // Add the input entries at the beginning of the array and remove\n    // as many elements from the array before inserting the original entries\n\n\n    const entries = replaceInFront(sliced, missingSourceEntries);\n    const logId = entries[entries.length - 1].id;\n    return {\n      logId,\n      entries\n    };\n  }\n\n}\n\nmodule.exports = LogIO;","map":null,"metadata":{},"sourceType":"script"}