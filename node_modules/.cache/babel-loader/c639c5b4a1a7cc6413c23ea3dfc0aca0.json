{"ast":null,"code":"'use strict';\n\nvar _regeneratorRuntime = require(\"/Users/fabianmolina/Documents/1hive/tulip-frontend/node_modules/babel-preset-react-app/node_modules/@babel/runtime/regenerator\");\n\nvar _asyncToGenerator = require(\"/Users/fabianmolina/Documents/1hive/tulip-frontend/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/asyncToGenerator\");\n\nvar _classCallCheck = require(\"/Users/fabianmolina/Documents/1hive/tulip-frontend/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/classCallCheck\");\n\nvar _createClass = require(\"/Users/fabianmolina/Documents/1hive/tulip-frontend/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/createClass\");\n\nvar pMap = require('p-map');\n\nvar pDoWhilst = require('p-do-whilst');\n\nvar Entry = require('./entry');\n\nvar hasItems = function hasItems(arr) {\n  return arr && arr.length > 0;\n};\n\nvar EntryIO = /*#__PURE__*/function () {\n  function EntryIO() {\n    _classCallCheck(this, EntryIO);\n  }\n\n  _createClass(EntryIO, null, [{\n    key: \"fetchParallel\",\n    value: // Fetch log graphs in parallel\n    function () {\n      var _fetchParallel = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2(ipfs, hashes, _ref) {\n        var length, _ref$exclude, exclude, timeout, concurrency, onProgressCallback, fetchOne, concatArrays, flatten, res;\n\n        return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n          while (1) {\n            switch (_context2.prev = _context2.next) {\n              case 0:\n                length = _ref.length, _ref$exclude = _ref.exclude, exclude = _ref$exclude === void 0 ? [] : _ref$exclude, timeout = _ref.timeout, concurrency = _ref.concurrency, onProgressCallback = _ref.onProgressCallback;\n\n                fetchOne = /*#__PURE__*/function () {\n                  var _ref2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee(hash) {\n                    return _regeneratorRuntime.wrap(function _callee$(_context) {\n                      while (1) {\n                        switch (_context.prev = _context.next) {\n                          case 0:\n                            return _context.abrupt(\"return\", EntryIO.fetchAll(ipfs, hash, {\n                              length: length,\n                              exclude: exclude,\n                              timeout: timeout,\n                              onProgressCallback: onProgressCallback,\n                              concurrency: concurrency\n                            }));\n\n                          case 1:\n                          case \"end\":\n                            return _context.stop();\n                        }\n                      }\n                    }, _callee);\n                  }));\n\n                  return function fetchOne(_x4) {\n                    return _ref2.apply(this, arguments);\n                  };\n                }();\n\n                concatArrays = function concatArrays(arr1, arr2) {\n                  return arr1.concat(arr2);\n                };\n\n                flatten = function flatten(arr) {\n                  return arr.reduce(concatArrays, []);\n                };\n\n                _context2.next = 6;\n                return pMap(hashes, fetchOne, {\n                  concurrency: Math.max(concurrency || hashes.length, 1)\n                });\n\n              case 6:\n                res = _context2.sent;\n                return _context2.abrupt(\"return\", flatten(res));\n\n              case 8:\n              case \"end\":\n                return _context2.stop();\n            }\n          }\n        }, _callee2);\n      }));\n\n      function fetchParallel(_x, _x2, _x3) {\n        return _fetchParallel.apply(this, arguments);\n      }\n\n      return fetchParallel;\n    }()\n    /**\n     * Fetch log entries\n     *\n     * @param {IPFS} [ipfs] An IPFS instance\n     * @param {string} [hash] Multihash of the entry to fetch\n     * @param {string} [parent] Parent of the node to be fetched\n     * @param {Object} [all] Entries to skip\n     * @param {Number} [amount=-1] How many entries to fetch\n     * @param {Number} [depth=0] Current depth of the recursion\n     * @param {function(hash, entry, parent, depth)} onProgressCallback\n     * @returns {Promise<Array<Entry>>}\n     */\n\n  }, {\n    key: \"fetchAll\",\n    value: function () {\n      var _fetchAll = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee6(ipfs, hashes) {\n        var _ref3,\n            _ref3$length,\n            length,\n            _ref3$exclude,\n            exclude,\n            timeout,\n            onProgressCallback,\n            onStartProgressCallback,\n            _ref3$concurrency,\n            concurrency,\n            _ref3$delay,\n            delay,\n            result,\n            cache,\n            loadingCache,\n            loadingQueue,\n            running,\n            maxClock,\n            minClock,\n            loadingQueueHasMore,\n            addToLoadingQueue,\n            getNextFromQueue,\n            addToExcludeCache,\n            fetchEntry,\n            _processQueue,\n            _args6 = arguments;\n\n        return _regeneratorRuntime.wrap(function _callee6$(_context6) {\n          while (1) {\n            switch (_context6.prev = _context6.next) {\n              case 0:\n                _ref3 = _args6.length > 2 && _args6[2] !== undefined ? _args6[2] : {}, _ref3$length = _ref3.length, length = _ref3$length === void 0 ? -1 : _ref3$length, _ref3$exclude = _ref3.exclude, exclude = _ref3$exclude === void 0 ? [] : _ref3$exclude, timeout = _ref3.timeout, onProgressCallback = _ref3.onProgressCallback, onStartProgressCallback = _ref3.onStartProgressCallback, _ref3$concurrency = _ref3.concurrency, concurrency = _ref3$concurrency === void 0 ? 32 : _ref3$concurrency, _ref3$delay = _ref3.delay, delay = _ref3$delay === void 0 ? 0 : _ref3$delay;\n                result = [];\n                cache = {};\n                loadingCache = {};\n                loadingQueue = Array.isArray(hashes) ? {\n                  0: hashes.slice()\n                } : {\n                  0: [hashes]\n                };\n                running = 0; // keep track of how many entries are being fetched at any time\n\n                maxClock = 0; // keep track of the latest clock time during load\n\n                minClock = 0; // keep track of the minimum clock time during load\n                // Does the loading queue have more to process?\n\n                loadingQueueHasMore = function loadingQueueHasMore() {\n                  return Object.values(loadingQueue).find(hasItems) !== undefined;\n                }; // Add a multihash to the loading queue\n\n\n                addToLoadingQueue = function addToLoadingQueue(e, idx) {\n                  if (!loadingCache[e]) {\n                    if (!loadingQueue[idx]) loadingQueue[idx] = [];\n\n                    if (!loadingQueue[idx].includes(e)) {\n                      loadingQueue[idx].push(e);\n                    }\n\n                    loadingCache[e] = true;\n                  }\n                }; // Get the next items to process from the loading queue\n\n\n                getNextFromQueue = function getNextFromQueue() {\n                  var length = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 1;\n\n                  var getNext = function getNext(res, key, idx) {\n                    var nextItems = loadingQueue[key];\n\n                    while (nextItems.length > 0 && res.length < length) {\n                      var hash = nextItems.shift();\n                      res.push(hash);\n                    }\n\n                    if (nextItems.length === 0) {\n                      delete loadingQueue[key];\n                    }\n\n                    return res;\n                  };\n\n                  return Object.keys(loadingQueue).reduce(getNext, []);\n                }; // Add entries that we don't need to fetch to the \"cache\"\n\n\n                addToExcludeCache = function addToExcludeCache(e) {\n                  cache[e.hash] = true;\n                }; // Fetch one entry and add it to the results\n\n\n                fetchEntry = /*#__PURE__*/function () {\n                  var _ref4 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee4(hash) {\n                    return _regeneratorRuntime.wrap(function _callee4$(_context4) {\n                      while (1) {\n                        switch (_context4.prev = _context4.next) {\n                          case 0:\n                            if (!(!hash || cache[hash])) {\n                              _context4.next = 2;\n                              break;\n                            }\n\n                            return _context4.abrupt(\"return\");\n\n                          case 2:\n                            return _context4.abrupt(\"return\", new Promise(function (resolve, reject) {\n                              // Resolve the promise after a timeout (if given) in order to\n                              // not get stuck loading a block that is unreachable\n                              var timer = timeout && timeout > 0 ? setTimeout(function () {\n                                console.warn(\"Warning: Couldn't fetch entry '\".concat(hash, \"', request timed out (\").concat(timeout, \"ms)\"));\n                                resolve();\n                              }, timeout) : null;\n\n                              var addToResults = function addToResults(entry) {\n                                if (Entry.isEntry(entry)) {\n                                  var ts = entry.clock.time; // Update min/max clocks\n\n                                  maxClock = Math.max(maxClock, ts);\n                                  minClock = result.length > 0 ? Math.min(result[result.length - 1].clock.time, minClock) : maxClock;\n                                  var isLater = result.length >= length && ts >= minClock;\n\n                                  var calculateIndex = function calculateIndex(idx) {\n                                    return maxClock - ts + (idx + 1) * idx;\n                                  }; // Add the entry to the results if\n                                  // 1) we're fetching all entries\n                                  // 2) results is not filled yet\n                                  // the clock of the entry is later than current known minimum clock time\n\n\n                                  if (length < 0 || result.length < length || isLater) {\n                                    result.push(entry);\n                                    cache[hash] = true;\n\n                                    if (onProgressCallback) {\n                                      onProgressCallback(hash, entry, result.length, result.length);\n                                    }\n                                  }\n\n                                  if (length < 0) {\n                                    // If we're fetching all entries (length === -1), adds nexts and refs to the queue\n                                    entry.next.forEach(addToLoadingQueue);\n                                    if (entry.refs) entry.refs.forEach(addToLoadingQueue);\n                                  } else {\n                                    // If we're fetching entries up to certain length,\n                                    // fetch the next if result is filled up, to make sure we \"check\"\n                                    // the next entry if its clock is later than what we have in the result\n                                    if (result.length < length || ts > minClock || ts === minClock && !cache[entry.hash]) {\n                                      entry.next.forEach(function (e) {\n                                        return addToLoadingQueue(e, calculateIndex(0));\n                                      });\n                                    }\n\n                                    if (entry.refs && result.length + entry.refs.length <= length) {\n                                      entry.refs.forEach(function (e, i) {\n                                        return addToLoadingQueue(e, calculateIndex(i));\n                                      });\n                                    }\n                                  }\n                                }\n                              };\n\n                              if (onStartProgressCallback) {\n                                onStartProgressCallback(hash, null, 0, result.length);\n                              } // Load the entry\n\n\n                              Entry.fromMultihash(ipfs, hash).then( /*#__PURE__*/function () {\n                                var _ref5 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee3(entry) {\n                                  var sleep;\n                                  return _regeneratorRuntime.wrap(function _callee3$(_context3) {\n                                    while (1) {\n                                      switch (_context3.prev = _context3.next) {\n                                        case 0:\n                                          _context3.prev = 0;\n                                          // Add it to the results\n                                          addToResults(entry); // Simulate network latency (for debugging purposes)\n\n                                          if (!(delay > 0)) {\n                                            _context3.next = 6;\n                                            break;\n                                          }\n\n                                          sleep = function sleep() {\n                                            var ms = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 0;\n                                            return new Promise(function (resolve) {\n                                              return setTimeout(resolve, ms);\n                                            });\n                                          };\n\n                                          _context3.next = 6;\n                                          return sleep(delay);\n\n                                        case 6:\n                                          resolve();\n                                          _context3.next = 12;\n                                          break;\n\n                                        case 9:\n                                          _context3.prev = 9;\n                                          _context3.t0 = _context3[\"catch\"](0);\n                                          reject(_context3.t0);\n\n                                        case 12:\n                                          _context3.prev = 12;\n                                          clearTimeout(timer);\n                                          return _context3.finish(12);\n\n                                        case 15:\n                                        case \"end\":\n                                          return _context3.stop();\n                                      }\n                                    }\n                                  }, _callee3, null, [[0, 9, 12, 15]]);\n                                }));\n\n                                return function (_x8) {\n                                  return _ref5.apply(this, arguments);\n                                };\n                              }()).catch(reject);\n                            }));\n\n                          case 3:\n                          case \"end\":\n                            return _context4.stop();\n                        }\n                      }\n                    }, _callee4);\n                  }));\n\n                  return function fetchEntry(_x7) {\n                    return _ref4.apply(this, arguments);\n                  };\n                }(); // One loop of processing the loading queue\n\n\n                _processQueue = /*#__PURE__*/function () {\n                  var _ref6 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee5() {\n                    var nexts;\n                    return _regeneratorRuntime.wrap(function _callee5$(_context5) {\n                      while (1) {\n                        switch (_context5.prev = _context5.next) {\n                          case 0:\n                            if (!(running < concurrency)) {\n                              _context5.next = 6;\n                              break;\n                            }\n\n                            nexts = getNextFromQueue(concurrency);\n                            running += nexts.length;\n                            _context5.next = 5;\n                            return pMap(nexts, fetchEntry);\n\n                          case 5:\n                            running -= nexts.length;\n\n                          case 6:\n                          case \"end\":\n                            return _context5.stop();\n                        }\n                      }\n                    }, _callee5);\n                  }));\n\n                  return function _processQueue() {\n                    return _ref6.apply(this, arguments);\n                  };\n                }(); // Add entries to exclude from processing to the cache before we start\n\n\n                exclude.forEach(addToExcludeCache); // Fetch entries\n\n                _context6.next = 17;\n                return pDoWhilst(_processQueue, loadingQueueHasMore);\n\n              case 17:\n                return _context6.abrupt(\"return\", result);\n\n              case 18:\n              case \"end\":\n                return _context6.stop();\n            }\n          }\n        }, _callee6);\n      }));\n\n      function fetchAll(_x5, _x6) {\n        return _fetchAll.apply(this, arguments);\n      }\n\n      return fetchAll;\n    }()\n  }]);\n\n  return EntryIO;\n}();\n\nmodule.exports = EntryIO;","map":null,"metadata":{},"sourceType":"script"}