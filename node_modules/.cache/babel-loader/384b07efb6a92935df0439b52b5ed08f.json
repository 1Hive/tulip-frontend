{"ast":null,"code":"'use strict';\n\nconst path = require('path');\n\nconst EventEmitter = require('events').EventEmitter;\n\nconst mapSeries = require('p-each-series');\n\nconst {\n  default: PQueue\n} = require('p-queue');\n\nconst Log = require('ipfs-log');\n\nconst Entry = Log.Entry;\n\nconst Index = require('./Index');\n\nconst Replicator = require('./Replicator');\n\nconst ReplicationInfo = require('./replication-info');\n\nconst Logger = require('logplease');\n\nconst logger = Logger.create('orbit-db.store', {\n  color: Logger.Colors.Blue\n});\nLogger.setLogLevel('ERROR');\n\nconst io = require('orbit-db-io');\n\nconst DefaultOptions = {\n  Index: Index,\n  maxHistory: -1,\n  fetchEntryTimeout: null,\n  referenceCount: 32,\n  replicationConcurrency: 128,\n  syncLocal: false,\n  sortFn: undefined\n};\n\nclass Store {\n  constructor(ipfs, identity, address, options) {\n    if (!identity) {\n      throw new Error('Identity required');\n    } // Set the options\n\n\n    const opts = Object.assign({}, DefaultOptions);\n    Object.assign(opts, options);\n    this.options = opts; // Default type\n\n    this._type = 'store'; // Create IDs, names and paths\n\n    this.id = address.toString();\n    this.identity = identity;\n    this.address = address;\n    this.dbname = address.path || '';\n    this.events = new EventEmitter();\n    this.remoteHeadsPath = path.join(this.id, '_remoteHeads');\n    this.localHeadsPath = path.join(this.id, '_localHeads');\n    this.snapshotPath = path.join(this.id, 'snapshot');\n    this.queuePath = path.join(this.id, 'queue');\n    this.manifestPath = path.join(this.id, '_manifest'); // External dependencies\n\n    this._ipfs = ipfs;\n    this._cache = options.cache; // Access mapping\n\n    const defaultAccess = {\n      canAppend: entry => entry.identity.publicKey === identity.publicKey\n    };\n    this.access = options.accessController || defaultAccess; // Create the operations log\n\n    this._oplog = new Log(this._ipfs, this.identity, {\n      logId: this.id,\n      access: this.access,\n      sortFn: this.options.sortFn\n    }); // _addOperation queue\n\n    this._opqueue = new PQueue({\n      concurrency: 1\n    }); // Create the index\n\n    this._index = new this.options.Index(this.address.root); // Replication progress info\n\n    this._replicationStatus = new ReplicationInfo(); // Statistics\n\n    this._stats = {\n      snapshot: {\n        bytesLoaded: -1\n      },\n      syncRequestsReceieved: 0\n    };\n\n    try {\n      this._replicator = new Replicator(this, this.options.replicationConcurrency); // For internal backwards compatibility,\n      // to be removed in future releases\n\n      this._loader = this._replicator;\n\n      this._replicator.on('load.added', entry => {\n        // Update the latest entry state (latest is the entry with largest clock time)\n        this._replicationStatus.queued++;\n\n        this._recalculateReplicationMax(entry.clock ? entry.clock.time : 0); // logger.debug(`<replicate>`)\n\n\n        this.events.emit('replicate', this.address.toString(), entry);\n      });\n\n      this._replicator.on('load.progress', (id, hash, entry, have, bufferedLength) => {\n        if (this._replicationStatus.buffered > bufferedLength) {\n          this._recalculateReplicationProgress(this.replicationStatus.progress + bufferedLength);\n        } else {\n          this._recalculateReplicationProgress(this._oplog.length + bufferedLength);\n        }\n\n        this._replicationStatus.buffered = bufferedLength;\n\n        this._recalculateReplicationMax(this.replicationStatus.progress); // logger.debug(`<replicate.progress>`)\n\n\n        this.events.emit('replicate.progress', this.address.toString(), hash, entry, this.replicationStatus.progress, this.replicationStatus.max);\n      });\n\n      const onLoadCompleted = async (logs, have) => {\n        try {\n          for (const log of logs) {\n            await this._oplog.join(log);\n          }\n\n          this._replicationStatus.queued -= logs.length;\n          this._replicationStatus.buffered = this._replicator._buffer.length;\n          await this._updateIndex(); // only store heads that has been verified and merges\n\n          const heads = this._oplog.heads;\n          await this._cache.set(this.remoteHeadsPath, heads);\n          logger.debug(`Saved heads ${heads.length} [${heads.map(e => e.hash).join(', ')}]`); // logger.debug(`<replicated>`)\n\n          this.events.emit('replicated', this.address.toString(), logs.length);\n        } catch (e) {\n          console.error(e);\n        }\n      };\n\n      this._replicator.on('load.end', onLoadCompleted);\n    } catch (e) {\n      console.error('Store Error:', e);\n    }\n\n    this.events.on('replicated.progress', (address, hash, entry, progress, have) => {\n      this._procEntry(entry);\n    });\n    this.events.on('write', (address, entry, heads) => {\n      this._procEntry(entry);\n    });\n  }\n\n  get all() {\n    return Array.isArray(this._index._index) ? this._index._index : Object.keys(this._index._index).map(e => this._index._index[e]);\n  }\n\n  get index() {\n    return this._index._index;\n  }\n\n  get type() {\n    return this._type;\n  }\n\n  get key() {\n    return this._key;\n  }\n  /**\n   * Returns the database's current replication status information\n   * @return {[Object]} [description]\n   */\n\n\n  get replicationStatus() {\n    return this._replicationStatus;\n  }\n\n  setIdentity(identity) {\n    this.identity = identity;\n\n    this._oplog.setIdentity(identity);\n  }\n\n  async close() {\n    if (this.options.onClose) {\n      await this.options.onClose(this);\n    }\n\n    await this._opqueue.onIdle(); // Replicator teardown logic\n\n    this._replicator.stop(); // Reset replication statistics\n\n\n    this._replicationStatus.reset(); // Reset database statistics\n\n\n    this._stats = {\n      snapshot: {\n        bytesLoaded: -1\n      },\n      syncRequestsReceieved: 0\n    }; // Remove all event listeners\n\n    for (var event in this.events._events) {\n      this.events.removeAllListeners(event);\n    } // Database is now closed\n    // TODO: afaik we don't use 'closed' event anymore,\n    // to be removed in future releases\n\n\n    this.events.emit('closed', this.address.toString());\n    return Promise.resolve();\n  }\n  /**\n   * Drops a database and removes local data\n   * @return {[None]}\n   */\n\n\n  async drop() {\n    if (this.options.onDrop) {\n      await this.options.onDrop(this);\n    }\n\n    await this._cache.del(this.localHeadsPath);\n    await this._cache.del(this.remoteHeadsPath);\n    await this._cache.del(this.snapshotPath);\n    await this._cache.del(this.queuePath);\n    await this._cache.del(this.manifestPath);\n    await this.close(); // Reset\n\n    this._index = new this.options.Index(this.address.root);\n    this._oplog = new Log(this._ipfs, this.identity, {\n      logId: this.id,\n      access: this.access,\n      sortFn: this.options.sortFn\n    });\n    this._cache = this.options.cache;\n  }\n\n  async load(amount, opts = {}) {\n    if (typeof amount === 'object') {\n      opts = amount;\n      amount = undefined;\n    }\n\n    amount = amount || this.options.maxHistory;\n    const fetchEntryTimeout = opts.fetchEntryTimeout || this.options.fetchEntryTimeout;\n\n    if (this.options.onLoad) {\n      await this.options.onLoad(this);\n    }\n\n    const localHeads = (await this._cache.get(this.localHeadsPath)) || [];\n    const remoteHeads = (await this._cache.get(this.remoteHeadsPath)) || [];\n    const heads = localHeads.concat(remoteHeads);\n\n    if (heads.length > 0) {\n      this.events.emit('load', this.address.toString(), heads);\n    } // Update the replication status from the heads\n\n\n    heads.forEach(h => this._recalculateReplicationMax(h.clock.time)); // Load the log\n\n    const log = await Log.fromEntryHash(this._ipfs, this.identity, heads.map(e => e.hash), {\n      logId: this._oplog.id,\n      access: this.access,\n      sortFn: this.options.sortFn,\n      length: amount,\n      exclude: this._oplog.values,\n      onProgressCallback: this._onLoadProgress.bind(this),\n      timeout: fetchEntryTimeout\n    }); // Join the log with the existing log\n\n    await this._oplog.join(log, amount); // Update the index\n\n    if (heads.length > 0) {\n      await this._updateIndex();\n    }\n\n    this.events.emit('ready', this.address.toString(), this._oplog.heads);\n  }\n\n  async sync(heads) {\n    this._stats.syncRequestsReceieved += 1;\n    logger.debug(`Sync request #${this._stats.syncRequestsReceieved} ${heads.length}`);\n\n    if (heads.length === 0) {\n      return;\n    } // To simulate network latency, uncomment this line\n    // and comment out the rest of the function\n    // That way the object (received as head message from pubsub)\n    // doesn't get written to IPFS and so when the Replicator is fetching\n    // the log, it'll fetch it from the network instead from the disk.\n    // return this._replicator.load(heads)\n\n\n    const saveToIpfs = async head => {\n      if (!head) {\n        console.warn(\"Warning: Given input entry was 'null'.\");\n        return Promise.resolve(null);\n      }\n\n      const identityProvider = this.identity.provider;\n      if (!identityProvider) throw new Error('Identity-provider is required, cannot verify entry');\n      const canAppend = await this.access.canAppend(head, identityProvider);\n\n      if (!canAppend) {\n        console.warn('Warning: Given input entry is not allowed in this log and was discarded (no write access).');\n        return Promise.resolve(null);\n      }\n\n      const logEntry = Entry.toEntry(head);\n      const hash = await io.write(this._ipfs, Entry.getWriteFormat(logEntry), logEntry, {\n        links: Entry.IPLD_LINKS,\n        onlyHash: true\n      });\n\n      if (hash !== head.hash) {\n        console.warn('\"WARNING! Head hash didn\\'t match the contents');\n      }\n\n      return head;\n    };\n\n    const saved = await mapSeries(heads, saveToIpfs);\n    await this._replicator.load(saved.filter(e => e !== null));\n\n    if (this._replicator._buffer.length || Object.values(this._replicator._queue).length) {\n      return new Promise(resolve => {\n        const progressHandler = (address, hash, entry, progress, have) => {\n          if (progress === have) {\n            this.events.off('replicate.progress', progressHandler);\n            this.events.once('replicated', resolve);\n          }\n        };\n\n        this.events.on('replicate.progress', progressHandler);\n      });\n    }\n  }\n\n  loadMoreFrom(amount, entries) {\n    this._replicator.load(entries);\n  }\n\n  async saveSnapshot() {\n    const unfinished = this._replicator.getQueue();\n\n    const snapshotData = this._oplog.toSnapshot();\n\n    const buf = Buffer.from(JSON.stringify({\n      id: snapshotData.id,\n      heads: snapshotData.heads,\n      size: snapshotData.values.length,\n      values: snapshotData.values,\n      type: this.type\n    }));\n    const snapshot = await this._ipfs.add(buf);\n    snapshot.hash = snapshot.cid.toString(); // js-ipfs >= 0.41, ipfs.add results contain a cid property (a CID instance) instead of a string hash property\n\n    await this._cache.set(this.snapshotPath, snapshot);\n    await this._cache.set(this.queuePath, unfinished);\n    logger.debug(`Saved snapshot: ${snapshot.hash}, queue length: ${unfinished.length}`);\n    return [snapshot];\n  }\n\n  async loadFromSnapshot(onProgressCallback) {\n    if (this.options.onLoad) {\n      await this.options.onLoad(this);\n    }\n\n    this.events.emit('load', this.address.toString()); // TODO emits inconsistent params, missing heads param\n\n    const maxClock = (res, val) => Math.max(res, val.clock.time);\n\n    const queue = await this._cache.get(this.queuePath);\n    this.sync(queue || []);\n    const snapshot = await this._cache.get(this.snapshotPath);\n\n    if (snapshot) {\n      const chunks = [];\n\n      for await (const chunk of this._ipfs.cat(snapshot.hash)) {\n        chunks.push(chunk);\n      }\n\n      const buffer = Buffer.concat(chunks);\n      const snapshotData = JSON.parse(buffer.toString());\n\n      const onProgress = (hash, entry, count, total) => {\n        this._recalculateReplicationStatus(count, entry.clock.time);\n\n        this._onLoadProgress(hash, entry);\n      }; // Fetch the entries\n      // Timeout 1 sec to only load entries that are already fetched (in order to not get stuck at loading)\n\n\n      this._recalculateReplicationMax(snapshotData.values.reduce(maxClock, 0));\n\n      if (snapshotData) {\n        const log = await Log.fromJSON(this._ipfs, this.identity, snapshotData, {\n          access: this.access,\n          sortFn: this.options.sortFn,\n          length: -1,\n          timeout: 1000,\n          onProgressCallback: onProgress\n        });\n        await this._oplog.join(log);\n        await this._updateIndex();\n        this.events.emit('replicated', this.address.toString()); // TODO: inconsistent params, count param not emited\n      }\n\n      this.events.emit('ready', this.address.toString(), this._oplog.heads);\n    } else {\n      throw new Error(`Snapshot for ${this.address} not found!`);\n    }\n\n    return this;\n  }\n\n  async _updateIndex() {\n    this._recalculateReplicationMax();\n\n    await this._index.updateIndex(this._oplog);\n\n    this._recalculateReplicationProgress();\n  }\n\n  async syncLocal() {\n    const localHeads = (await this._cache.get(this.localHeadsPath)) || [];\n    const remoteHeads = (await this._cache.get(this.remoteHeadsPath)) || [];\n    const heads = localHeads.concat(remoteHeads);\n\n    for (let i = 0; i < heads.length; i++) {\n      const head = heads[i];\n\n      if (!this._oplog.heads.includes(head)) {\n        await this.load();\n        break;\n      }\n    }\n  }\n\n  async _addOperation(data, {\n    onProgressCallback,\n    pin = false\n  } = {}) {\n    async function addOperation() {\n      if (this._oplog) {\n        // check local cache?\n        if (this.options.syncLocal) {\n          await this.syncLocal();\n        }\n\n        const entry = await this._oplog.append(data, this.options.referenceCount, pin);\n\n        this._recalculateReplicationStatus(this.replicationStatus.progress + 1, entry.clock.time);\n\n        await this._cache.set(this.localHeadsPath, [entry]);\n        await this._updateIndex();\n        this.events.emit('write', this.address.toString(), entry, this._oplog.heads);\n        if (onProgressCallback) onProgressCallback(entry);\n        return entry.hash;\n      }\n    }\n\n    return this._opqueue.add(addOperation.bind(this));\n  }\n\n  _addOperationBatch(data, batchOperation, lastOperation, onProgressCallback) {\n    throw new Error('Not implemented!');\n  }\n\n  _procEntry(entry) {\n    var {\n      payload,\n      hash\n    } = entry;\n    var {\n      op\n    } = payload;\n\n    if (op) {\n      this.events.emit(`log.op.${op}`, this.address.toString(), hash, payload);\n    } else {\n      this.events.emit('log.op.none', this.address.toString(), hash, payload);\n    }\n\n    this.events.emit('log.op', op, this.address.toString(), hash, payload);\n  }\n\n  _onLoadProgress(hash, entry, progress, total) {\n    this._recalculateReplicationStatus(progress, total);\n\n    this.events.emit('load.progress', this.address.toString(), hash, entry, this.replicationStatus.progress, this.replicationStatus.max);\n  }\n  /* Replication Status state updates */\n\n\n  _recalculateReplicationProgress(max) {\n    this._replicationStatus.progress = Math.max.apply(null, [this._replicationStatus.progress, this._oplog.length, max || 0]);\n\n    this._recalculateReplicationMax(this.replicationStatus.progress);\n  }\n\n  _recalculateReplicationMax(max) {\n    this._replicationStatus.max = Math.max.apply(null, [this._replicationStatus.max, this._oplog.length, max || 0]);\n  }\n\n  _recalculateReplicationStatus(maxProgress, maxTotal) {\n    this._recalculateReplicationProgress(maxProgress);\n\n    this._recalculateReplicationMax(maxTotal);\n  }\n\n}\n\nmodule.exports = Store;\nmodule.exports.DefaultOptions = DefaultOptions;","map":null,"metadata":{},"sourceType":"script"}