{"ast":null,"code":"'use strict';\n\nvar _objectSpread = require(\"/Users/fabianmolina/Documents/1hive/tulip-frontend/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/objectSpread2\");\n\nvar _regeneratorRuntime = require(\"/Users/fabianmolina/Documents/1hive/tulip-frontend/node_modules/babel-preset-react-app/node_modules/@babel/runtime/regenerator\");\n\nvar _asyncToGenerator = require(\"/Users/fabianmolina/Documents/1hive/tulip-frontend/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/asyncToGenerator\");\n\nvar _require = require('ipld-dag-pb'),\n    DAGNode = _require.DAGNode;\n\nvar Bucket = require('hamt-sharding/src/bucket');\n\nvar DirSharded = require('ipfs-unixfs-importer/src/dir-sharded');\n\nvar log = require('debug')('ipfs:mfs:core:utils:hamt-utils');\n\nvar UnixFS = require('ipfs-unixfs');\n\nvar mc = require('multicodec');\n\nvar mh = require('multihashes');\n\nvar last = require('it-last');\n\nvar _require2 = require('buffer'),\n    Buffer = _require2.Buffer;\n\nvar updateHamtDirectory = /*#__PURE__*/function () {\n  var _ref = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee(context, links, bucket, options) {\n    var data, node, dir, hashAlg, parent, cid;\n    return _regeneratorRuntime.wrap(function _callee$(_context) {\n      while (1) {\n        switch (_context.prev = _context.next) {\n          case 0:\n            // update parent with new bit field\n            data = Buffer.from(bucket._children.bitField().reverse());\n            node = UnixFS.unmarshal(options.parent.Data);\n            dir = new UnixFS({\n              type: 'hamt-sharded-directory',\n              data: data,\n              fanout: bucket.tableSize(),\n              hashType: DirSharded.hashFn.code,\n              mode: node.mode,\n              mtime: node.mtime\n            });\n            hashAlg = mh.names[options.hashAlg];\n            parent = new DAGNode(dir.marshal(), links);\n            _context.next = 7;\n            return context.ipld.put(parent, mc.DAG_PB, {\n              cidVersion: options.cidVersion,\n              hashAlg: hashAlg,\n              onlyHash: !options.flush\n            });\n\n          case 7:\n            cid = _context.sent;\n            return _context.abrupt(\"return\", {\n              node: parent,\n              cid: cid,\n              size: parent.size\n            });\n\n          case 9:\n          case \"end\":\n            return _context.stop();\n        }\n      }\n    }, _callee);\n  }));\n\n  return function updateHamtDirectory(_x, _x2, _x3, _x4) {\n    return _ref.apply(this, arguments);\n  };\n}();\n\nvar recreateHamtLevel = /*#__PURE__*/function () {\n  var _ref2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2(links, rootBucket, parentBucket, positionAtParent) {\n    var bucket;\n    return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n      while (1) {\n        switch (_context2.prev = _context2.next) {\n          case 0:\n            // recreate this level of the HAMT\n            bucket = new Bucket({\n              hashFn: DirSharded.hashFn,\n              hash: parentBucket ? parentBucket._options.hash : undefined\n            }, parentBucket, positionAtParent);\n\n            if (parentBucket) {\n              parentBucket._putObjectAt(positionAtParent, bucket);\n            }\n\n            _context2.next = 4;\n            return addLinksToHamtBucket(links, bucket, rootBucket);\n\n          case 4:\n            return _context2.abrupt(\"return\", bucket);\n\n          case 5:\n          case \"end\":\n            return _context2.stop();\n        }\n      }\n    }, _callee2);\n  }));\n\n  return function recreateHamtLevel(_x5, _x6, _x7, _x8) {\n    return _ref2.apply(this, arguments);\n  };\n}();\n\nvar addLinksToHamtBucket = /*#__PURE__*/function () {\n  var _ref3 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee3(links, bucket, rootBucket) {\n    return _regeneratorRuntime.wrap(function _callee3$(_context3) {\n      while (1) {\n        switch (_context3.prev = _context3.next) {\n          case 0:\n            _context3.next = 2;\n            return Promise.all(links.map(function (link) {\n              if (link.Name.length === 2) {\n                var pos = parseInt(link.Name, 16);\n\n                bucket._putObjectAt(pos, new Bucket({\n                  hashFn: DirSharded.hashFn\n                }, bucket, pos));\n\n                return Promise.resolve();\n              }\n\n              return (rootBucket || bucket).put(link.Name.substring(2), {\n                size: link.Tsize,\n                cid: link.Hash\n              });\n            }));\n\n          case 2:\n          case \"end\":\n            return _context3.stop();\n        }\n      }\n    }, _callee3);\n  }));\n\n  return function addLinksToHamtBucket(_x9, _x10, _x11) {\n    return _ref3.apply(this, arguments);\n  };\n}();\n\nvar toPrefix = function toPrefix(position) {\n  return position.toString('16').toUpperCase().padStart(2, '0').substring(0, 2);\n};\n\nvar generatePath = /*#__PURE__*/function () {\n  var _ref4 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee4(context, fileName, rootNode) {\n    var rootBucket, position, path, currentBucket, _loop, i, _ret;\n\n    return _regeneratorRuntime.wrap(function _callee4$(_context5) {\n      while (1) {\n        switch (_context5.prev = _context5.next) {\n          case 0:\n            _context5.next = 2;\n            return recreateHamtLevel(rootNode.Links, null, null, null);\n\n          case 2:\n            rootBucket = _context5.sent;\n            _context5.next = 5;\n            return rootBucket._findNewBucketAndPos(fileName);\n\n          case 5:\n            position = _context5.sent;\n            // the path to the root bucket\n            path = [{\n              bucket: position.bucket,\n              prefix: toPrefix(position.pos)\n            }];\n            currentBucket = position.bucket;\n\n            while (currentBucket !== rootBucket) {\n              path.push({\n                bucket: currentBucket,\n                prefix: toPrefix(currentBucket._posAtParent)\n              });\n              currentBucket = currentBucket._parent;\n            }\n\n            path.reverse();\n            path[0].node = rootNode; // load DAGNode for each path segment\n\n            _loop = /*#__PURE__*/_regeneratorRuntime.mark(function _loop(i) {\n              var segment, link, node, _position, nextSegment;\n\n              return _regeneratorRuntime.wrap(function _loop$(_context4) {\n                while (1) {\n                  switch (_context4.prev = _context4.next) {\n                    case 0:\n                      segment = path[i]; // find prefix in links\n\n                      link = segment.node.Links.filter(function (link) {\n                        return link.Name.substring(0, 2) === segment.prefix;\n                      }).pop(); // entry was not in shard\n\n                      if (link) {\n                        _context4.next = 5;\n                        break;\n                      }\n\n                      // reached bottom of tree, file will be added to the current bucket\n                      log(\"Link \".concat(segment.prefix).concat(fileName, \" will be added\")); // return path\n\n                      return _context4.abrupt(\"return\", \"continue\");\n\n                    case 5:\n                      if (!(link.Name === \"\".concat(segment.prefix).concat(fileName))) {\n                        _context4.next = 8;\n                        break;\n                      }\n\n                      log(\"Link \".concat(segment.prefix).concat(fileName, \" will be replaced\")); // file already existed, file will be added to the current bucket\n                      // return path\n\n                      return _context4.abrupt(\"return\", \"continue\");\n\n                    case 8:\n                      // found subshard\n                      log(\"Found subshard \".concat(segment.prefix));\n                      _context4.next = 11;\n                      return context.ipld.get(link.Hash);\n\n                    case 11:\n                      node = _context4.sent;\n\n                      if (path[i + 1]) {\n                        _context4.next = 21;\n                        break;\n                      }\n\n                      log(\"Loaded new subshard \".concat(segment.prefix));\n                      _context4.next = 16;\n                      return recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n\n                    case 16:\n                      _context4.next = 18;\n                      return rootBucket._findNewBucketAndPos(fileName);\n\n                    case 18:\n                      _position = _context4.sent;\n                      // i--\n                      path.push({\n                        bucket: _position.bucket,\n                        prefix: toPrefix(_position.pos),\n                        node: node\n                      });\n                      return _context4.abrupt(\"return\", \"continue\");\n\n                    case 21:\n                      nextSegment = path[i + 1]; // add intermediate links to bucket\n\n                      _context4.next = 24;\n                      return addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket);\n\n                    case 24:\n                      nextSegment.node = node;\n\n                    case 25:\n                    case \"end\":\n                      return _context4.stop();\n                  }\n                }\n              }, _loop);\n            });\n            i = 0;\n\n          case 13:\n            if (!(i < path.length)) {\n              _context5.next = 21;\n              break;\n            }\n\n            return _context5.delegateYield(_loop(i), \"t0\", 15);\n\n          case 15:\n            _ret = _context5.t0;\n\n            if (!(_ret === \"continue\")) {\n              _context5.next = 18;\n              break;\n            }\n\n            return _context5.abrupt(\"continue\", 18);\n\n          case 18:\n            i++;\n            _context5.next = 13;\n            break;\n\n          case 21:\n            _context5.next = 23;\n            return rootBucket.put(fileName, true);\n\n          case 23:\n            path.reverse();\n            return _context5.abrupt(\"return\", {\n              rootBucket: rootBucket,\n              path: path\n            });\n\n          case 25:\n          case \"end\":\n            return _context5.stop();\n        }\n      }\n    }, _callee4);\n  }));\n\n  return function generatePath(_x12, _x13, _x14) {\n    return _ref4.apply(this, arguments);\n  };\n}();\n\nvar createShard = /*#__PURE__*/function () {\n  var _ref5 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee5(context, contents, options) {\n    var shard, i;\n    return _regeneratorRuntime.wrap(function _callee5$(_context6) {\n      while (1) {\n        switch (_context6.prev = _context6.next) {\n          case 0:\n            shard = new DirSharded({\n              root: true,\n              dir: true,\n              parent: null,\n              parentKey: null,\n              path: '',\n              dirty: true,\n              flat: false,\n              mtime: options.mtime,\n              mode: options.mode\n            }, _objectSpread(_objectSpread({}, options), {}, {\n              codec: 'dag-pb'\n            }));\n            i = 0;\n\n          case 2:\n            if (!(i < contents.length)) {\n              _context6.next = 8;\n              break;\n            }\n\n            _context6.next = 5;\n            return shard._bucket.put(contents[i].name, {\n              size: contents[i].size,\n              cid: contents[i].cid\n            });\n\n          case 5:\n            i++;\n            _context6.next = 2;\n            break;\n\n          case 8:\n            return _context6.abrupt(\"return\", last(shard.flush('', context.block, null)));\n\n          case 9:\n          case \"end\":\n            return _context6.stop();\n        }\n      }\n    }, _callee5);\n  }));\n\n  return function createShard(_x15, _x16, _x17) {\n    return _ref5.apply(this, arguments);\n  };\n}();\n\nmodule.exports = {\n  generatePath: generatePath,\n  updateHamtDirectory: updateHamtDirectory,\n  recreateHamtLevel: recreateHamtLevel,\n  addLinksToHamtBucket: addLinksToHamtBucket,\n  toPrefix: toPrefix,\n  createShard: createShard\n};","map":null,"metadata":{},"sourceType":"script"}