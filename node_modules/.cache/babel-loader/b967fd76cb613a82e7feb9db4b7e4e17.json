{"ast":null,"code":"'use strict';\n\nconst {\n  DAGNode\n} = require('ipld-dag-pb');\n\nconst Bucket = require('hamt-sharding/src/bucket');\n\nconst DirSharded = require('ipfs-unixfs-importer/src/dir-sharded');\n\nconst log = require('debug')('ipfs:mfs:core:utils:hamt-utils');\n\nconst UnixFS = require('ipfs-unixfs');\n\nconst mc = require('multicodec');\n\nconst mh = require('multihashes');\n\nconst last = require('it-last');\n\nconst {\n  Buffer\n} = require('buffer');\n\nconst updateHamtDirectory = async (context, links, bucket, options) => {\n  // update parent with new bit field\n  const data = Buffer.from(bucket._children.bitField().reverse());\n  const node = UnixFS.unmarshal(options.parent.Data);\n  const dir = new UnixFS({\n    type: 'hamt-sharded-directory',\n    data,\n    fanout: bucket.tableSize(),\n    hashType: DirSharded.hashFn.code,\n    mode: node.mode,\n    mtime: node.mtime\n  });\n  const hashAlg = mh.names[options.hashAlg];\n  const parent = new DAGNode(dir.marshal(), links);\n  const cid = await context.ipld.put(parent, mc.DAG_PB, {\n    cidVersion: options.cidVersion,\n    hashAlg,\n    onlyHash: !options.flush\n  });\n  return {\n    node: parent,\n    cid,\n    size: parent.size\n  };\n};\n\nconst recreateHamtLevel = async (links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hashFn: DirSharded.hashFn,\n    hash: parentBucket ? parentBucket._options.hash : undefined\n  }, parentBucket, positionAtParent);\n\n  if (parentBucket) {\n    parentBucket._putObjectAt(positionAtParent, bucket);\n  }\n\n  await addLinksToHamtBucket(links, bucket, rootBucket);\n  return bucket;\n};\n\nconst addLinksToHamtBucket = async (links, bucket, rootBucket) => {\n  await Promise.all(links.map(link => {\n    if (link.Name.length === 2) {\n      const pos = parseInt(link.Name, 16);\n\n      bucket._putObjectAt(pos, new Bucket({\n        hashFn: DirSharded.hashFn\n      }, bucket, pos));\n\n      return Promise.resolve();\n    }\n\n    return (rootBucket || bucket).put(link.Name.substring(2), {\n      size: link.Tsize,\n      cid: link.Hash\n    });\n  }));\n};\n\nconst toPrefix = position => {\n  return position.toString('16').toUpperCase().padStart(2, '0').substring(0, 2);\n};\n\nconst generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateHamtLevel(rootNode.Links, null, null, null);\n  const position = await rootBucket._findNewBucketAndPos(fileName); // the path to the root bucket\n\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }];\n  let currentBucket = position.bucket;\n\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    });\n    currentBucket = currentBucket._parent;\n  }\n\n  path.reverse();\n  path[0].node = rootNode; // load DAGNode for each path segment\n\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i]; // find prefix in links\n\n    const link = segment.node.Links.filter(link => link.Name.substring(0, 2) === segment.prefix).pop(); // entry was not in shard\n\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`); // return path\n\n      continue;\n    } // found entry\n\n\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`); // file already existed, file will be added to the current bucket\n      // return path\n\n      continue;\n    } // found subshard\n\n\n    log(`Found subshard ${segment.prefix}`);\n    const node = await context.ipld.get(link.Hash); // subshard hasn't been loaded, descend to the next level of the HAMT\n\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`);\n      await recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n      const position = await rootBucket._findNewBucketAndPos(fileName); // i--\n\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      });\n      continue;\n    }\n\n    const nextSegment = path[i + 1]; // add intermediate links to bucket\n\n    await addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket);\n    nextSegment.node = node;\n  }\n\n  await rootBucket.put(fileName, true);\n  path.reverse();\n  return {\n    rootBucket,\n    path\n  };\n};\n\nconst createShard = async (context, contents, options) => {\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: null,\n    parentKey: null,\n    path: '',\n    dirty: true,\n    flat: false,\n    mtime: options.mtime,\n    mode: options.mode\n  }, { ...options,\n    codec: 'dag-pb'\n  });\n\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    });\n  }\n\n  return last(shard.flush('', context.block, null));\n};\n\nmodule.exports = {\n  generatePath,\n  updateHamtDirectory,\n  recreateHamtLevel,\n  addLinksToHamtBucket,\n  toPrefix,\n  createShard\n};","map":null,"metadata":{},"sourceType":"script"}