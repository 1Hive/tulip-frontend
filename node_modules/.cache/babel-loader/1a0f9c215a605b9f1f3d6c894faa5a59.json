{"ast":null,"code":"'use strict';\n\nconst pMap = require('p-map');\n\nconst GSet = require('./g-set');\n\nconst Entry = require('./entry');\n\nconst LogIO = require('./log-io');\n\nconst LogError = require('./log-errors');\n\nconst Clock = require('./lamport-clock');\n\nconst Sorting = require('./log-sorting');\n\nconst {\n  LastWriteWins,\n  NoZeroes\n} = Sorting;\n\nconst AccessController = require('./default-access-controller');\n\nconst {\n  isDefined,\n  findUniques\n} = require('./utils');\n\nconst EntryIndex = require('./entry-index');\n\nconst randomId = () => new Date().getTime().toString();\n\nconst getHash = e => e.hash;\n\nconst flatMap = (res, acc) => res.concat(acc);\n\nconst getNextPointers = entry => entry.next;\n\nconst maxClockTimeReducer = (res, acc) => Math.max(res, acc.clock.time);\n\nconst uniqueEntriesReducer = (res, acc) => {\n  res[acc.hash] = acc;\n  return res;\n};\n/**\n * Log.\n *\n * @description\n * Log implements a G-Set CRDT and adds ordering.\n *\n * From:\n * \"A comprehensive study of Convergent and Commutative Replicated Data Types\"\n * https://hal.inria.fr/inria-00555588\n */\n\n\nclass Log extends GSet {\n  /**\n   * Create a new Log instance\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Object} identity Identity (https://github.com/orbitdb/orbit-db-identity-provider/blob/master/src/identity.js)\n   * @param {Object} options\n   * @param {string} options.logId ID of the log\n   * @param {Object} options.access AccessController (./default-access-controller)\n   * @param {Array<Entry>} options.entries An Array of Entries from which to create the log\n   * @param {Array<Entry>} options.heads Set the heads of the log\n   * @param {Clock} options.clock Set the clock of the log\n   * @param {Function} options.sortFn The sort function - by default LastWriteWins\n   * @return {Log} The log instance\n   */\n  constructor(ipfs, identity, {\n    logId,\n    access,\n    entries,\n    heads,\n    clock,\n    sortFn,\n    concurrency\n  } = {}) {\n    if (!isDefined(ipfs)) {\n      throw LogError.IPFSNotDefinedError();\n    }\n\n    if (!isDefined(identity)) {\n      throw new Error('Identity is required');\n    }\n\n    if (!isDefined(access)) {\n      access = new AccessController();\n    }\n\n    if (isDefined(entries) && !Array.isArray(entries)) {\n      throw new Error('\\'entries\\' argument must be an array of Entry instances');\n    }\n\n    if (isDefined(heads) && !Array.isArray(heads)) {\n      throw new Error('\\'heads\\' argument must be an array');\n    }\n\n    if (!isDefined(sortFn)) {\n      sortFn = LastWriteWins;\n    }\n\n    super();\n    this._sortFn = NoZeroes(sortFn);\n    this._storage = ipfs;\n    this._id = logId || randomId(); // Access Controller\n\n    this._access = access; // Identity\n\n    this._identity = identity; // Add entries to the internal cache\n\n    const uniqueEntries = (entries || []).reduce(uniqueEntriesReducer, {});\n    this._entryIndex = new EntryIndex(uniqueEntries);\n    entries = Object.values(uniqueEntries) || []; // Set heads if not passed as an argument\n\n    heads = heads || Log.findHeads(entries);\n    this._headsIndex = heads.reduce(uniqueEntriesReducer, {}); // Index of all next pointers in this log\n\n    this._nextsIndex = {};\n\n    const addToNextsIndex = e => e.next.forEach(a => this._nextsIndex[a] = e.hash);\n\n    entries.forEach(addToNextsIndex); // Set the length, we calculate the length manually internally\n\n    this._length = entries.length; // Set the clock\n\n    const maxTime = Math.max(clock ? clock.time : 0, this.heads.reduce(maxClockTimeReducer, 0)); // Take the given key as the clock id is it's a Key instance,\n    // otherwise if key was given, take whatever it is,\n    // and if it was null, take the given id as the clock id\n\n    this._clock = new Clock(this._identity.publicKey, maxTime);\n    this.joinConcurrency = concurrency || 16;\n  }\n  /**\n   * Returns the ID of the log.\n   * @returns {string}\n   */\n\n\n  get id() {\n    return this._id;\n  }\n  /**\n   * Returns the clock of the log.\n   * @returns {string}\n   */\n\n\n  get clock() {\n    return this._clock;\n  }\n  /**\n   * Returns the length of the log.\n   * @return {number} Length\n   */\n\n\n  get length() {\n    return this._length;\n  }\n  /**\n   * Returns the values in the log.\n   * @returns {Array<Entry>}\n   */\n\n\n  get values() {\n    return Object.values(this.traverse(this.heads)).reverse();\n  }\n  /**\n   * Returns an array of heads as hashes.\n   * @returns {Array<string>}\n   */\n\n\n  get heads() {\n    return Object.values(this._headsIndex).sort(this._sortFn).reverse();\n  }\n  /**\n   * Returns an array of Entry objects that reference entries which\n   * are not in the log currently.\n   * @returns {Array<Entry>}\n   */\n\n\n  get tails() {\n    return Log.findTails(this.values);\n  }\n  /**\n   * Returns an array of hashes that are referenced by entries which\n   * are not in the log currently.\n   * @returns {Array<string>} Array of hashes\n   */\n\n\n  get tailHashes() {\n    return Log.findTailHashes(this.values);\n  }\n  /**\n   * Set the identity for the log\n   * @param {Identity} [identity] The identity to be set\n   */\n\n\n  setIdentity(identity) {\n    this._identity = identity; // Find the latest clock from the heads\n\n    const time = Math.max(this.clock.time, this.heads.reduce(maxClockTimeReducer, 0));\n    this._clock = new Clock(this._identity.publicKey, time);\n  }\n  /**\n   * Find an entry.\n   * @param {string} [hash] The hashes of the entry\n   * @returns {Entry|undefined}\n   */\n\n\n  get(hash) {\n    return this._entryIndex.get(hash);\n  }\n  /**\n   * Checks if a entry is part of the log\n   * @param {string} hash The hash of the entry\n   * @returns {boolean}\n   */\n\n\n  has(entry) {\n    return this._entryIndex.get(entry.hash || entry) !== undefined;\n  }\n\n  traverse(rootEntries, amount = -1, endHash) {\n    // Sort the given given root entries and use as the starting stack\n    let stack = rootEntries.sort(this._sortFn).reverse(); // Cache for checking if we've processed an entry already\n\n    let traversed = {}; // End result\n\n    const result = {};\n    let count = 0; // Named function for getting an entry from the log\n\n    const getEntry = e => this.get(e); // Add an entry to the stack and traversed nodes index\n\n\n    const addToStack = entry => {\n      // If we've already processed the entry, don't add it to the stack\n      if (!entry || traversed[entry.hash]) {\n        return;\n      } // Add the entry in front of the stack and sort\n\n\n      stack = [entry, ...stack].sort(this._sortFn).reverse(); // Add to the cache of processed entries\n\n      traversed[entry.hash] = true;\n    };\n\n    const addEntry = rootEntry => {\n      result[rootEntry.hash] = rootEntry;\n      traversed[rootEntry.hash] = true;\n      count++;\n    }; // Start traversal\n    // Process stack until it's empty (traversed the full log)\n    // or when we have the requested amount of entries\n    // If requested entry amount is -1, traverse all\n\n\n    while (stack.length > 0 && (count < amount || amount < 0)) {\n      // eslint-disable-line no-unmodified-loop-condition\n      // Get the next element from the stack\n      const entry = stack.shift(); // Add to the result\n\n      addEntry(entry); // If it is the specified end hash, break out of the while loop\n\n      if (endHash && endHash === entry.hash) break; // Add entry's next references to the stack\n\n      const entries = entry.next.map(getEntry);\n      const defined = entries.filter(isDefined);\n      defined.forEach(addToStack);\n    }\n\n    stack = [];\n    traversed = {}; // End result\n\n    return result;\n  }\n  /**\n   * Append an entry to the log.\n   * @param {Entry} entry Entry to add\n   * @return {Log} New Log containing the appended value\n   */\n\n\n  async append(data, pointerCount = 1, pin = false) {\n    // Update the clock (find the latest clock)\n    const newTime = Math.max(this.clock.time, this.heads.reduce(maxClockTimeReducer, 0)) + 1;\n    this._clock = new Clock(this.clock.id, newTime);\n    const all = Object.values(this.traverse(this.heads, Math.max(pointerCount, this.heads.length))); // If pointer count is 4, returns 2\n    // If pointer count is 8, returns 3 references\n    // If pointer count is 512, returns 9 references\n    // If pointer count is 2048, returns 11 references\n\n    const getEveryPow2 = maxDistance => {\n      const entries = new Set();\n\n      for (let i = 1; i <= maxDistance; i *= 2) {\n        const index = Math.min(i - 1, all.length - 1);\n        entries.add(all[index]);\n      }\n\n      return entries;\n    };\n\n    const references = getEveryPow2(Math.min(pointerCount, all.length)); // Always include the last known reference\n\n    if (all.length < pointerCount && all[all.length - 1]) {\n      references.add(all[all.length - 1]);\n    } // Create the next pointers from heads\n\n\n    const nexts = Object.keys(this.heads.reverse().reduce(uniqueEntriesReducer, {}));\n\n    const isNext = e => !nexts.includes(e); // Delete the heads from the refs\n\n\n    const refs = Array.from(references).map(getHash).filter(isNext); // @TODO: Split Entry.create into creating object, checking permission, signing and then posting to IPFS\n    // Create the entry and add it to the internal cache\n\n    const entry = await Entry.create(this._storage, this._identity, this.id, data, nexts, this.clock, refs, pin);\n    const canAppend = await this._access.canAppend(entry, this._identity.provider);\n\n    if (!canAppend) {\n      throw new Error(`Could not append entry, key \"${this._identity.id}\" is not allowed to write to the log`);\n    }\n\n    this._entryIndex.set(entry.hash, entry);\n\n    nexts.forEach(e => this._nextsIndex[e] = entry.hash);\n    this._headsIndex = {};\n    this._headsIndex[entry.hash] = entry; // Update the length\n\n    this._length++;\n    return entry;\n  }\n  /*\n   * Creates a javscript iterator over log entries\n   *\n   * @param {Object} options\n   * @param {string|Array} options.gt Beginning hash of the iterator, non-inclusive\n   * @param {string|Array} options.gte Beginning hash of the iterator, inclusive\n   * @param {string|Array} options.lt Ending hash of the iterator, non-inclusive\n   * @param {string|Array} options.lte Ending hash of the iterator, inclusive\n   * @param {amount} options.amount Number of entried to return to / from the gte / lte hash\n   * @returns {Symbol.Iterator} Iterator object containing log entries\n   *\n   * @examples\n   *\n   * (async () => {\n   *   log1 = new Log(ipfs, testIdentity, { logId: 'X' })\n   *\n   *   for (let i = 0; i <= 100; i++) {\n   *     await log1.append('entry' + i)\n   *   }\n   *\n   *   let it = log1.iterator({\n   *     lte: 'zdpuApFd5XAPkCTmSx7qWQmQzvtdJPtx2K5p9to6ytCS79bfk',\n   *     amount: 10\n   *   })\n   *\n   *   [...it].length // 10\n   * })()\n   *\n   *\n   */\n\n\n  iterator({\n    gt = undefined,\n    gte = undefined,\n    lt = undefined,\n    lte = undefined,\n    amount = -1\n  } = {}) {\n    if (amount === 0) return function* () {}();\n    if (typeof lte === 'string') lte = [this.get(lte)];\n    if (typeof lt === 'string') lt = [this.get(this.get(lt).next)];\n    if (lte && !Array.isArray(lte)) throw LogError.LtOrLteMustBeStringOrArray();\n    if (lt && !Array.isArray(lt)) throw LogError.LtOrLteMustBeStringOrArray();\n    const start = (lte || lt || this.heads).filter(isDefined);\n    const endHash = gte ? this.get(gte).hash : gt ? this.get(gt).hash : null;\n    const count = endHash ? -1 : amount || -1;\n    const entries = this.traverse(start, count, endHash);\n    let entryValues = Object.values(entries); // Strip off last entry if gt is non-inclusive\n\n    if (gt) entryValues.pop(); // Deal with the amount argument working backwards from gt/gte\n\n    if ((gt || gte) && amount > -1) {\n      entryValues = entryValues.slice(entryValues.length - amount, entryValues.length);\n    }\n\n    return function* () {\n      for (const i in entryValues) {\n        yield entryValues[i];\n      }\n    }();\n  }\n  /**\n   * Join two logs.\n   *\n   * Joins another log into this one.\n   *\n   * @param {Log} log Log to join with this Log\n   * @param {number} [size=-1] Max size of the joined log\n   * @returns {Promise<Log>} This Log instance\n   * @example\n   * await log1.join(log2)\n   */\n\n\n  async join(log, size = -1) {\n    if (!isDefined(log)) throw LogError.LogNotDefinedError();\n    if (!Log.isLog(log)) throw LogError.NotALogError();\n    if (this.id !== log.id) return; // Get the difference of the logs\n\n    const newItems = Log.difference(log, this);\n    const identityProvider = this._identity.provider; // Verify if entries are allowed to be added to the log and throws if\n    // there's an invalid entry\n\n    const permitted = async entry => {\n      const canAppend = await this._access.canAppend(entry, identityProvider);\n\n      if (!canAppend) {\n        throw new Error(`Could not append entry, key \"${entry.identity.id}\" is not allowed to write to the log`);\n      }\n    }; // Verify signature for each entry and throws if there's an invalid signature\n\n\n    const verify = async entry => {\n      const isValid = await Entry.verify(identityProvider, entry);\n      const publicKey = entry.identity ? entry.identity.publicKey : entry.key;\n      if (!isValid) throw new Error(`Could not validate signature \"${entry.sig}\" for entry \"${entry.hash}\" and key \"${publicKey}\"`);\n    };\n\n    const entriesToJoin = Object.values(newItems);\n    await pMap(entriesToJoin, async e => {\n      await permitted(e);\n      await verify(e);\n    }, {\n      concurrency: this.joinConcurrency\n    }); // Update the internal next pointers index\n\n    const addToNextsIndex = e => {\n      const entry = this.get(e.hash);\n      if (!entry) this._length++;\n      /* istanbul ignore else */\n\n      e.next.forEach(a => this._nextsIndex[a] = e.hash);\n    };\n\n    Object.values(newItems).forEach(addToNextsIndex); // Update the internal entry index\n\n    this._entryIndex.add(newItems); // Merge the heads\n\n\n    const notReferencedByNewItems = e => !nextsFromNewItems.find(a => a === e.hash);\n\n    const notInCurrentNexts = e => !this._nextsIndex[e.hash];\n\n    const nextsFromNewItems = Object.values(newItems).map(getNextPointers).reduce(flatMap, []);\n    const mergedHeads = Log.findHeads(Object.values(Object.assign({}, this._headsIndex, log._headsIndex))).filter(notReferencedByNewItems).filter(notInCurrentNexts).reduce(uniqueEntriesReducer, {});\n    this._headsIndex = mergedHeads; // Slice to the requested size\n\n    if (size > -1) {\n      let tmp = this.values;\n      tmp = tmp.slice(-size);\n      this._entryIndex = null;\n      this._entryIndex = new EntryIndex(tmp.reduce(uniqueEntriesReducer, {}));\n      this._headsIndex = Log.findHeads(tmp).reduce(uniqueEntriesReducer, {});\n      this._length = this._entryIndex.length;\n    } // Find the latest clock from the heads\n\n\n    const maxClock = Object.values(this._headsIndex).reduce(maxClockTimeReducer, 0);\n    this._clock = new Clock(this.clock.id, Math.max(this.clock.time, maxClock));\n    return this;\n  }\n  /**\n   * Get the log in JSON format.\n   * @returns {Object} An object with the id and heads properties\n   */\n\n\n  toJSON() {\n    return {\n      id: this.id,\n      heads: this.heads.sort(this._sortFn) // default sorting\n      .reverse() // we want the latest as the first element\n      .map(getHash) // return only the head hashes\n\n    };\n  }\n  /**\n   * Get the log in JSON format as a snapshot.\n   * @returns {Object} An object with the id, heads and value properties\n   */\n\n\n  toSnapshot() {\n    return {\n      id: this.id,\n      heads: this.heads,\n      values: this.values\n    };\n  }\n  /**\n   * Get the log as a Buffer.\n   * @returns {Buffer}\n   */\n\n\n  toBuffer() {\n    return Buffer.from(JSON.stringify(this.toJSON()));\n  }\n  /**\n   * Returns the log entries as a formatted string.\n   * @returns {string}\n   * @example\n   * two\n   * └─one\n   *   └─three\n   */\n\n\n  toString(payloadMapper) {\n    return this.values.slice().reverse().map((e, idx) => {\n      const parents = Entry.findChildren(e, this.values);\n      const len = parents.length;\n      let padding = new Array(Math.max(len - 1, 0));\n      padding = len > 1 ? padding.fill('  ') : padding;\n      padding = len > 0 ? padding.concat(['└─']) : padding;\n      /* istanbul ignore next */\n\n      return padding.join('') + (payloadMapper ? payloadMapper(e.payload) : e.payload);\n    }).join('\\n');\n  }\n  /**\n   * Check whether an object is a Log instance.\n   * @param {Object} log An object to check\n   * @returns {boolean}\n   */\n\n\n  static isLog(log) {\n    return log.id !== undefined && log.heads !== undefined && log._entryIndex !== undefined;\n  }\n  /**\n   * Get the log's multihash.\n   * @returns {Promise<string>} Multihash of the Log as Base58 encoded string.\n   */\n\n\n  toMultihash({\n    format\n  } = {}) {\n    return LogIO.toMultihash(this._storage, this, {\n      format\n    });\n  }\n  /**\n   * Create a log from a hashes.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Identity} identity The identity instance\n   * @param {string} hash The log hash\n   * @param {Object} options\n   * @param {AccessController} options.access The access controller instance\n   * @param {number} options.length How many items to include in the log\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   * @param {Function} options.sortFn The sort function - by default LastWriteWins\n   * @returns {Promise<Log>}\n   */\n\n\n  static async fromMultihash(ipfs, identity, hash, {\n    access,\n    length = -1,\n    exclude = [],\n    timeout,\n    concurrency,\n    sortFn,\n    onProgressCallback\n  } = {}) {\n    // TODO: need to verify the entries with 'key'\n    const {\n      logId,\n      entries,\n      heads\n    } = await LogIO.fromMultihash(ipfs, hash, {\n      length,\n      exclude,\n      timeout,\n      onProgressCallback,\n      concurrency,\n      sortFn\n    });\n    return new Log(ipfs, identity, {\n      logId,\n      access,\n      entries,\n      heads,\n      sortFn\n    });\n  }\n  /**\n   * Create a log from a single entry's hash.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Identity} identity The identity instance\n   * @param {string} hash The entry's hash\n   * @param {Object} options\n   * @param {string} options.logId The ID of the log\n   * @param {AccessController} options.access The access controller instance\n   * @param {number} options.length How many entries to include in the log\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   * @param {Function} options.sortFn The sort function - by default LastWriteWins\n   * @return {Promise<Log>} New Log\n   */\n\n\n  static async fromEntryHash(ipfs, identity, hash, {\n    logId,\n    access,\n    length = -1,\n    exclude = [],\n    timeout,\n    concurrency,\n    sortFn,\n    onProgressCallback\n  } = {}) {\n    // TODO: need to verify the entries with 'key'\n    const {\n      entries\n    } = await LogIO.fromEntryHash(ipfs, hash, {\n      length,\n      exclude,\n      timeout,\n      concurrency,\n      onProgressCallback\n    });\n    return new Log(ipfs, identity, {\n      logId,\n      access,\n      entries,\n      sortFn\n    });\n  }\n  /**\n   * Create a log from a Log Snapshot JSON.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Identity} identity The identity instance\n   * @param {Object} json Log snapshot as JSON object\n   * @param {Object} options\n   * @param {AccessController} options.access The access controller instance\n   * @param {number} options.length How many entries to include in the log\n   * @param {function(hash, entry, parent, depth)} [options.onProgressCallback]\n   * @param {Function} options.sortFn The sort function - by default LastWriteWins\n   * @return {Promise<Log>} New Log\n   */\n\n\n  static async fromJSON(ipfs, identity, json, {\n    access,\n    length = -1,\n    timeout,\n    sortFn,\n    onProgressCallback\n  } = {}) {\n    // TODO: need to verify the entries with 'key'\n    const {\n      logId,\n      entries\n    } = await LogIO.fromJSON(ipfs, json, {\n      length,\n      timeout,\n      onProgressCallback\n    });\n    return new Log(ipfs, identity, {\n      logId,\n      access,\n      entries,\n      sortFn\n    });\n  }\n  /**\n   * Create a new log from an Entry instance.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Identity} identity The identity instance\n   * @param {Entry|Array<Entry>} sourceEntries An Entry or an array of entries to fetch a log from\n   * @param {Object} options\n   * @param {AccessController} options.access The access controller instance\n   * @param {number} options.length How many entries to include. Default: infinite.\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} [options.onProgressCallback]\n   * @param {Function} options.sortFn The sort function - by default LastWriteWins\n   * @return {Promise<Log>} New Log\n   */\n\n\n  static async fromEntry(ipfs, identity, sourceEntries, {\n    access,\n    length = -1,\n    exclude = [],\n    timeout,\n    concurrency,\n    sortFn,\n    onProgressCallback\n  } = {}) {\n    // TODO: need to verify the entries with 'key'\n    const {\n      logId,\n      entries\n    } = await LogIO.fromEntry(ipfs, sourceEntries, {\n      length,\n      exclude,\n      timeout,\n      concurrency,\n      onProgressCallback\n    });\n    return new Log(ipfs, identity, {\n      logId,\n      access,\n      entries,\n      sortFn\n    });\n  }\n  /**\n   * Find heads from a collection of entries.\n   *\n   * Finds entries that are the heads of this collection,\n   * ie. entries that are not referenced by other entries.\n   *\n   * @param {Array<Entry>} entries Entries to search heads from\n   * @returns {Array<Entry>}\n   */\n\n\n  static findHeads(entries) {\n    var indexReducer = (res, entry, idx, arr) => {\n      var addToResult = e => res[e] = entry.hash;\n\n      entry.next.forEach(addToResult);\n      return res;\n    };\n\n    var items = entries.reduce(indexReducer, {});\n\n    var exists = e => items[e.hash] === undefined;\n\n    var compareIds = (a, b) => a.clock.id > b.clock.id;\n\n    return entries.filter(exists).sort(compareIds);\n  } // Find entries that point to another entry that is not in the\n  // input array\n\n\n  static findTails(entries) {\n    // Reverse index { next -> entry }\n    var reverseIndex = {}; // Null index containing entries that have no parents (nexts)\n\n    var nullIndex = []; // Hashes for all entries for quick lookups\n\n    var hashes = {}; // Hashes of all next entries\n\n    var nexts = [];\n\n    var addToIndex = e => {\n      if (e.next.length === 0) {\n        nullIndex.push(e);\n      }\n\n      var addToReverseIndex = a => {\n        /* istanbul ignore else */\n        if (!reverseIndex[a]) reverseIndex[a] = [];\n        reverseIndex[a].push(e);\n      }; // Add all entries and their parents to the reverse index\n\n\n      e.next.forEach(addToReverseIndex); // Get all next references\n\n      nexts = nexts.concat(e.next); // Get the hashes of input entries\n\n      hashes[e.hash] = true;\n    }; // Create our indices\n\n\n    entries.forEach(addToIndex);\n\n    var addUniques = (res, entries, idx, arr) => res.concat(findUniques(entries, 'hash'));\n\n    var exists = e => hashes[e] === undefined;\n\n    var findFromReverseIndex = e => reverseIndex[e]; // Drop hashes that are not in the input entries\n\n\n    const tails = nexts // For every hash in nexts:\n    .filter(exists) // Remove undefineds and nulls\n    .map(findFromReverseIndex) // Get the Entry from the reverse index\n    .reduce(addUniques, []) // Flatten the result and take only uniques\n    .concat(nullIndex); // Combine with tails the have no next refs (ie. first-in-their-chain)\n\n    return findUniques(tails, 'hash').sort(Entry.compare);\n  } // Find the hashes to entries that are not in a collection\n  // but referenced by other entries\n\n\n  static findTailHashes(entries) {\n    var hashes = {};\n\n    var addToIndex = e => hashes[e.hash] = true;\n\n    var reduceTailHashes = (res, entry, idx, arr) => {\n      var addToResult = e => {\n        /* istanbul ignore else */\n        if (hashes[e] === undefined) {\n          res.splice(0, 0, e);\n        }\n      };\n\n      entry.next.reverse().forEach(addToResult);\n      return res;\n    };\n\n    entries.forEach(addToIndex);\n    return entries.reduce(reduceTailHashes, []);\n  }\n\n  static difference(a, b) {\n    const stack = Object.keys(a._headsIndex);\n    const traversed = {};\n    const res = {};\n\n    const pushToStack = hash => {\n      if (!traversed[hash] && !b.get(hash)) {\n        stack.push(hash);\n        traversed[hash] = true;\n      }\n    };\n\n    while (stack.length > 0) {\n      const hash = stack.shift();\n      const entry = a.get(hash);\n\n      if (entry && !b.get(hash) && entry.id === b.id) {\n        res[entry.hash] = entry;\n        traversed[entry.hash] = true;\n        entry.next.forEach(pushToStack);\n      }\n    }\n\n    return res;\n  }\n\n}\n\nmodule.exports = Log;\nmodule.exports.Sorting = Sorting;\nmodule.exports.Entry = Entry;\nmodule.exports.AccessController = AccessController;","map":null,"metadata":{},"sourceType":"script"}